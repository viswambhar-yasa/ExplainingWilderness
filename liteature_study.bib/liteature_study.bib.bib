
@article{bach_pixel-wise_2015,
	title = {On {Pixel}-{Wise} {Explanations} for {Non}-{Linear} {Classifier} {Decisions} by {Layer}-{Wise} {Relevance} {Propagation}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140},
	doi = {10.1371/journal.pone.0130140},
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	language = {en},
	number = {7},
	urldate = {2023-10-23},
	journal = {PLOS ONE},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
	month = jul,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Coding mechanisms, Imaging techniques, Kernel functions, Neural networks, Neurons, Sensory perception, Vision},
	pages = {e0130140},
	file = {Full Text PDF:files/2/Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf:application/pdf},
}

@misc{noauthor_layer-wise_nodate,
	title = {Layer-wise {Relevance} {Propagation} - 공돌이의 수학정리노트 ({Angelo}'s {Math} {Notes})},
	url = {https://angeloyeo.github.io/2019/08/17/Layerwise_Relevance_Propagation_en.html},
	urldate = {2023-10-23},
	file = {Layer-wise Relevance Propagation - 공돌이의 수학정리노트 (Angelo's Math Notes):files/4/Layerwise_Relevance_Propagation_en.html:text/html},
}

@article{montavon_methods_2018,
	title = {Methods for {Interpreting} and {Understanding} {Deep} {Neural} {Networks}},
	volume = {73},
	issn = {10512004},
	url = {http://arxiv.org/abs/1706.07979},
	doi = {10.1016/j.dsp.2017.10.011},
	abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most eﬃcient use of these techniques on real data. It also discusses a number of practical applications.},
	language = {en},
	urldate = {2023-10-23},
	journal = {Digital Signal Processing},
	author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
	month = feb,
	year = {2018},
	note = {arXiv:1706.07979 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--15},
	annote = {Comment: 14 pages, 10 figures},
	file = {Montavon et al. - 2018 - Methods for Interpreting and Understanding Deep Ne.pdf:files/6/Montavon et al. - 2018 - Methods for Interpreting and Understanding Deep Ne.pdf:application/pdf},
}

@misc{hammersborg_concept_2023,
	title = {Concept backpropagation: {An} {Explainable} {AI} approach for visualising learned concepts in neural network models},
	shorttitle = {Concept backpropagation},
	url = {http://arxiv.org/abs/2307.12601},
	doi = {10.48550/arXiv.2307.12601},
	abstract = {Neural network models are widely used in a variety of domains, often as black-box solutions, since they are not directly interpretable for humans. The field of explainable artificial intelligence aims at developing explanation methods to address this challenge, and several approaches have been developed over the recent years, including methods for investigating what type of knowledge these models internalise during the training process. Among these, the method of concept detection, investigates which {\textbackslash}emph\{concepts\} neural network models learn to represent in order to complete their tasks. In this work, we present an extension to the method of concept detection, named {\textbackslash}emph\{concept backpropagation\}, which provides a way of analysing how the information representing a given concept is internalised in a given neural network model. In this approach, the model input is perturbed in a manner guided by a trained concept probe for the described model, such that the concept of interest is maximised. This allows for the visualisation of the detected concept directly in the input space of the model, which in turn makes it possible to see what information the model depends on for representing the described concept. We present results for this method applied to a various set of input modalities, and discuss how our proposed method can be used to visualise what information trained concept probes use, and the degree as to which the representation of the probed concept is entangled within the neural network model itself.},
	urldate = {2023-10-23},
	publisher = {arXiv},
	author = {Hammersborg, Patrik and Strümke, Inga},
	month = jul,
	year = {2023},
	note = {arXiv:2307.12601 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/12/Hammersborg and Strümke - 2023 - Concept backpropagation An Explainable AI approac.pdf:application/pdf;arXiv.org Snapshot:files/13/2307.html:text/html},
}

@inproceedings{binder_layer-wise_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Layer-{Wise} {Relevance} {Propagation} for {Neural} {Networks} with {Local} {Renormalization} {Layers}},
	isbn = {978-3-319-44781-0},
	doi = {10.1007/978-3-319-44781-0_8},
	abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2016},
	publisher = {Springer International Publishing},
	author = {Binder, Alexander and Montavon, Grégoire and Lapuschkin, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
	editor = {Villa, Alessandro E.P. and Masulli, Paolo and Pons Rivero, Antonio Javier},
	year = {2016},
	keywords = {Neural networks, Image classification, Interpretability},
	pages = {63--71},
	file = {Submitted Version:files/15/Binder et al. - 2016 - Layer-Wise Relevance Propagation for Neural Networ.pdf:application/pdf},
}

@article{mcgrath_acquisition_2022,
	title = {Acquisition of chess knowledge in {AlphaZero}},
	volume = {119},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2206625119},
	doi = {10.1073/pnas.2206625119},
	abstract = {We analyze the knowledge acquired by AlphaZero, a neural network engine that learns chess solely by playing against itself yet becomes capable of outperforming human chess players. Although the system trains without access to human games or guidance, it appears to learn concepts analogous to those used by human chess players. We provide two lines of evidence. Linear probes applied to AlphaZero’s internal state enable us to quantify when and where such concepts are represented in the network. We also describe a behavioral analysis of opening play, including qualitative commentary by a former world chess champion.},
	number = {47},
	urldate = {2023-10-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {McGrath, Thomas and Kapishnikov, Andrei and Tomašev, Nenad and Pearce, Adam and Wattenberg, Martin and Hassabis, Demis and Kim, Been and Paquet, Ulrich and Kramnik, Vladimir},
	month = nov,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2206625119},
	file = {Full Text PDF:files/17/McGrath et al. - 2022 - Acquisition of chess knowledge in AlphaZero.pdf:application/pdf},
}

@article{achtibat_attribution_2023,
	title = {From attribution maps to human-understandable explanations through {Concept} {Relevance} {Propagation}},
	volume = {5},
	copyright = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00711-8},
	doi = {10.1038/s42256-023-00711-8},
	abstract = {The field of explainable artificial intelligence (XAI) aims to bring transparency to today’s powerful but opaque deep learning models. While local XAI methods explain individual predictions in the form of attribution maps, thereby identifying ‘where’ important features occur (but not providing information about ‘what’ they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of method thus provide only partial insights and leave the burden of interpreting the model’s reasoning to the user. Here we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the ‘where’ and ‘what’ questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model’s representation and reasoning through concept atlases, concept-composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision-making.},
	language = {en},
	number = {9},
	urldate = {2023-10-27},
	journal = {Nat Mach Intell},
	author = {Achtibat, Reduan and Dreyer, Maximilian and Eisenbraun, Ilona and Bosse, Sebastian and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
	month = sep,
	year = {2023},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Computer science, Statistics},
	pages = {1006--1019},
	file = {Full Text PDF:files/19/Achtibat et al. - 2023 - From attribution maps to human-understandable expl.pdf:application/pdf},
}

@article{kiyan_adaption_nodate,
	title = {Adaption of {Layerwise} {Relevance} {Propagation} for {Audio} {Applications}},
	language = {en},
	author = {Kiyan, Roman and Poschadel, Nils and Preihs, Stephan and Peissig, Jurgen},
	file = {Kiyan et al. - Adaption of Layerwise Relevance Propagation for Au.pdf:files/22/Kiyan et al. - Adaption of Layerwise Relevance Propagation for Au.pdf:application/pdf},
}

@misc{achtibat_where_2022,
	title = {From "{Where}" to "{What}": {Towards} {Human}-{Understandable} {Explanations} through {Concept} {Relevance} {Propagation}},
	shorttitle = {From "{Where}" to "{What}"},
	url = {http://arxiv.org/abs/2206.03208},
	doi = {10.48550/arXiv.2206.03208},
	abstract = {The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. Only few contemporary techniques aim at combining the principles behind both local and global XAI for obtaining more informative explanations. Those methods, however, are often limited to specific model architectures or impose additional requirements on training regimes or data and label availability, which renders the post-hoc application to arbitrarily pre-trained models practically impossible. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives of XAI and thus allows answering both the "where" and "what" questions for individual predictions, without additional constraints imposed. We further introduce the principle of Relevance Maximization for finding representative examples of encoded concepts based on their usefulness to the model. We thereby lift the dependency on the common practice of Activation Maximization and its limitations. We demonstrate the capabilities of our methods in various settings, showcasing that Concept Relevance Propagation and Relevance Maximization lead to more human interpretable explanations and provide deep insights into the model's representations and reasoning through concept atlases, concept composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision making.},
	urldate = {2023-11-23},
	publisher = {arXiv},
	author = {Achtibat, Reduan and Dreyer, Maximilian and Eisenbraun, Ilona and Bosse, Sebastian and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03208 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 79 pages (40 pages manuscript, 10 pages references, 29 pages appendix) 51 figures (26 in manuscript, 25 in appendix) 1 table (in appendix)},
	file = {arXiv Fulltext PDF:files/29/Achtibat et al. - 2022 - From Where to What Towards Human-Understandab.pdf:application/pdf;arXiv.org Snapshot:files/30/2206.html:text/html},
}

@incollection{montavon_layer-wise_2019,
	title = {Layer-{Wise} {Relevance} {Propagation}: {An} {Overview}},
	isbn = {978-3-030-28953-9},
	shorttitle = {Layer-{Wise} {Relevance} {Propagation}},
	abstract = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a ‘deep Taylor decomposition’, (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	author = {Montavon, Grégoire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and Müller, Klaus-Robert},
	month = sep,
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_10},
	note = {Journal Abbreviation: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	pages = {193--209},
	file = {Full Text PDF:files/34/Montavon et al. - 2019 - Layer-Wise Relevance Propagation An Overview.pdf:application/pdf},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Software} for {Dataset}-wide {XAI}: {From} {Local} {Explanations} to {Global} {Insights} with {Zennit}, {CoRelAy}, and {ViRelAy}},
	url = {https://www.researchgate.net/publication/352792272_Software_for_Dataset-wide_XAI_From_Local_Explanations_to_Global_Insights_with_Zennit_CoRelAy_and_ViRelAy},
	urldate = {2023-11-23},
}

@book{anders_software_2021,
	title = {Software for {Dataset}-wide {XAI}: {From} {Local} {Explanations} to {Global} {Insights} with {Zennit}, {CoRelAy}, and {ViRelAy}},
	shorttitle = {Software for {Dataset}-wide {XAI}},
	abstract = {Deep Neural Networks (DNNs) are known to be strong predictors, but their prediction strategies can rarely be understood. With recent advances in Explainable Artificial Intelligence , approaches are available to explore the reasoning behind those complex models' predictions. One class of approaches are post-hoc attribution methods, among which Layer-wise Relevance Propagation (LRP) shows high performance. However, the attempt at understanding a DNN's reasoning often stops at the attributions obtained for individual samples in input space, leaving the potential for deeper quantitative analyses untouched. As a manual analysis without the right tools is often unnecessarily labor intensive, we introduce three software packages targeted at scientists to explore model reasoning using attribution approaches and beyond: (1) Zennit - a highly customizable and intuitive attribution framework implementing LRP and related approaches in PyTorch, (2) CoRelAy - a framework to easily and quickly construct quantitative analysis pipelines for dataset-wide analyses of explanations, and (3) ViRelAy - a web-application to interactively explore data, attributions, and analysis results.},
	author = {Anders, Christopher and Neumann, David and Samek, Wojciech and Müller, Klaus-Robert and Lapuschkin, Sebastian},
	month = jun,
	year = {2021},
	file = {Full Text PDF:files/38/Anders et al. - 2021 - Software for Dataset-wide XAI From Local Explanat.pdf:application/pdf},
}

@misc{pahde_reveal_2023,
	title = {Reveal to {Revise}: {An} {Explainable} {AI} {Life} {Cycle} for {Iterative} {Bias} {Correction} of {Deep} {Models}},
	shorttitle = {Reveal to {Revise}},
	url = {http://arxiv.org/abs/2303.12641},
	abstract = {State-of-the-art machine learning models often learn spurious correlations embedded in the training data. This poses risks when deploying these models for high-stake decision-making, such as in medical applications like skin cancer detection. To tackle this problem, we propose Reveal to Revise (R2R), a framework entailing the entire eXplainable Artiﬁcial Intelligence (XAI) life cycle, enabling practitioners to iteratively identify, mitigate, and (re-)evaluate spurious model behavior with a minimal amount of human interaction. In the ﬁrst step (1), R2R reveals model weaknesses by ﬁnding outliers in attributions or through inspection of latent concepts learned by the model. Secondly (2), the responsible artifacts are detected and spatially localized in the input data, which is then leveraged to (3) revise the model behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model correction, and (4) (re-)evaluate the model’s performance and remaining sensitivity towards the artifact. Using two medical benchmark datasets for Melanoma detection and bone age estimation, we apply our R2R framework to VGG, ResNet and EﬃcientNet architectures and thereby reveal and correct real dataset-intrinsic artifacts, as well as synthetic variants in a controlled setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations to mitigate diﬀerent biases. Code is available on https://github.com/maxdreyer/Reveal2Revise.},
	language = {en},
	urldate = {2023-11-23},
	publisher = {arXiv},
	author = {Pahde, Frederik and Dreyer, Maximilian and Samek, Wojciech and Lapuschkin, Sebastian},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12641 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Pahde et al. - 2023 - Reveal to Revise An Explainable AI Life Cycle for.pdf:files/41/Pahde et al. - 2023 - Reveal to Revise An Explainable AI Life Cycle for.pdf:application/pdf},
}

@misc{stomberg_exploring_2022,
	title = {Exploring {Wilderness} {Characteristics} {Using} {Explainable} {Machine} {Learning} in {Satellite} {Imagery}},
	url = {http://arxiv.org/abs/2203.00379},
	abstract = {Wilderness areas oﬀer important ecological and social beneﬁts and there are urgent reasons to discover where their positive characteristics and ecological functions are present and able to ﬂourish. We apply a novel explainable machine learning technique to satellite images which show wild and anthropogenic areas in Fennoscandia. Occluding certain activations in an interpretable artiﬁcial neural network we complete a comprehensive sensitivity analysis regarding wild and anthropogenic characteristics. Our approach advances explainable machine learning for remote sensing, oﬀers opportunities for comprehensive analyses of existing wilderness, and has practical relevance for conservation eﬀorts.},
	language = {en},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Stomberg, Timo T. and Stone, Taylor and Leonhardt, Johannes and Weber, Immanuel and Roscher, Ribana},
	month = jul,
	year = {2022},
	note = {arXiv:2203.00379 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Stomberg et al. - 2022 - Exploring Wilderness Characteristics Using Explain.pdf:files/43/Stomberg et al. - 2022 - Exploring Wilderness Characteristics Using Explain.pdf:application/pdf},
}

@inproceedings{zhang_band_2017,
	address = {Huddersfield, United Kingdom},
	title = {Band selection in sentinel-2 satellite for agriculture applications},
	isbn = {978-0-7017-0260-1},
	url = {http://ieeexplore.ieee.org/document/8081990/},
	doi = {10.23919/IConAC.2017.8081990},
	abstract = {Various indices are used for assessing vegetation and soil properties in satellite remote sensing applications. Some indices, such as NDVI and NDWI, are defined based on the sensitivity and significance of specific bands. Nowadays, remote sensing capability with a good number of bands and high spatial resolution is available. Instead of classification based on indices, this paper explores direct classification using selected bands. Recently launched Sentinel-2A is adopted as a case study. Three methods are compared, where the first approach utilizes traditional indices and the latter two approaches adopt specific bands (Red, NIR, and SWIR) and full bands of on-board sensors, respectively. It is shown that a better classification performance can be achieved by directly using the three selected bands compared with the one using indices, while the use of all 13 bands can further improve the performance. Therefore, it is recommended the new approach can be applied for Sentinel-2A image analysis and other wide applications.},
	language = {en},
	urldate = {2023-11-24},
	booktitle = {2017 23rd {International} {Conference} on {Automation} and {Computing} ({ICAC})},
	publisher = {IEEE},
	author = {Zhang, Tianxiang and Su, Jinya and Liu, Cunjia and Chen, Wen-Hua and Liu, Hui and Liu, Guohai},
	month = sep,
	year = {2017},
	pages = {1--6},
	file = {Zhang et al. - 2017 - Band selection in sentinel-2 satellite for agricul.pdf:files/45/Zhang et al. - 2017 - Band selection in sentinel-2 satellite for agricul.pdf:application/pdf},
}

@misc{gisgeography_sentinel_2019,
	title = {Sentinel 2 {Bands} and {Combinations}},
	url = {https://gisgeography.com/sentinel-2-bands-combinations/},
	abstract = {Sentinel 2 has a total of 13 bands. Each band is 10, 20 or 60 meters in pixel size. This includes red, green, blue, near infrared and short infrared bands.},
	language = {en-US},
	urldate = {2023-11-24},
	journal = {GIS Geography},
	author = {GISGeography},
	month = apr,
	year = {2019},
	file = {Snapshot:files/48/sentinel-2-bands-combinations.html:text/html},
}

@article{kouki_forest_2001,
	title = {Forest {Fragmentation} in {Fennoscandia}: {Linking} {Habitat} {Requirements} of {Wood}-associated {Threatened} {Species} to {Landscape} and {Habitat} {Changes}},
	volume = {16},
	issn = {0282-7581},
	shorttitle = {Forest {Fragmentation} in {Fennoscandia}},
	url = {https://doi.org/10.1080/028275801300090564},
	doi = {10.1080/028275801300090564},
	abstract = {Fragmentation may occur simultaneously in different spatial and temporal scales. The ecological importance of fragmentation depends both on the scale of fragmentation and on the habitat requirements of the species. The fragmentation of old-growth forests is regarded as one of the most important causes for the recent decline of several forest-dwelling species in Fennoscandia. In Fennoscandia landscape-level fragmentation has proceeded differently in various areas. For example, in eastern Finland mature forest fragmentation was evident very early. By the 1800's, only one third of the landscape was covered by 150 yr old forests. This early fragmentation resulted mostly from the slash-and-burn cultivation practised widely in these areas. In the northern Fennoscandia, however, no such landscape changes have been found before the 1900's. During the 1900's fragmentation has accelerated. Recent results suggest that biological impacts of current landscape-level fragmentation of mature forests may have been overestimated, especially among invertebrate species. Specific habitat requirements of these groups are still poorly known. Instead of requiring mature forests, several wood-associated threatened species may require only dead wood that can occur in any successional stage, in both managed and protected areas. Such variability in the habitat requirements allows good opportunities for preservation of the threatened species. Results from current research indicate the need to restore and recreate natural fire-originated early successional stages where the amount of coarse woody debris is high. Promotion of coarse woody debris in young managed forests is potentially a very effective way to sustain populations of several threatened wood-associated species.},
	number = {sup003},
	urldate = {2023-11-25},
	journal = {Scandinavian Journal of Forest Research},
	author = {Kouki, Jari and Löfman, Satu and Martikainen, Petri and Rouvinen, Seppo and Uotila, Anneli},
	month = jan,
	year = {2001},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/028275801300090564},
	keywords = {Biodiversity, Coarse Woody Debris Cwd, Fennoscandia, Forest History, Fragmentation, Landscape, Old-GROWTH Forest, Restoration, Succession, Young Forest},
	pages = {27--37},
}

@article{yahya_comparison_2021,
	title = {Comparison of {Convolutional} {Neural} {Network} {Architectures} for {Face} {Mask} {Detection}},
	volume = {12},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=12&Issue=12&Code=IJACSA&SerialNo=83},
	doi = {10.14569/IJACSA.2021.0121283},
	abstract = {In 2020 World Health Organization (WHO) has declared that the Coronaviruses (COVID-19) pandemic is causing a worldwide health disaster. One of the most effective protections for reducing the spread of COVID-19 is by wearing a face mask in densely and close populated areas. In various countries, it has become mandatory to wear a face mask in public areas. The process of monitoring large numbers of individuals to comply with the new rule can be a challenging task. A costeffective method to monitor a large number of individuals to comply with this new law is through computer vision and Convolution Neural Network (CNN). This paper demonstrates the application of transfer learning on pre-trained CNN architectures namely; AlexNet, GoogleNet ResNet-18, ResNet-50, ResNet-101, to classify whether or not a person in the image is wearing a facemask. The number of training images are varied in order to compare the performance of these networks. It is found that AlexNet performed the worst and requires 400 training images to achieve Specificity, Accuracy, Precision, and F-score of more than 95\%. Whereas, GoogleNet and Resnet can achieve the same level of performance with 10 times fewer number of training images.},
	language = {en},
	number = {12},
	urldate = {2023-11-25},
	journal = {IJACSA},
	author = {Yahya, Siti Nadia and Ramli, Aizat Faiz and Nordin, Muhammad Noor and Basarudin, Hafiz and Abu, Mohd Azlan},
	year = {2021},
	file = {Yahya et al. - 2021 - Comparison of Convolutional Neural Network Archite.pdf:files/50/Yahya et al. - 2021 - Comparison of Convolutional Neural Network Archite.pdf:application/pdf},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
}

@inproceedings{krizhevsky_imagenet_2012-1,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
}

@inproceedings{yang_explaining_2018,
	address = {New York, NY},
	title = {Explaining {Therapy} {Predictions} with {Layer}-{Wise} {Relevance} {Propagation} in {Neural} {Networks}},
	isbn = {978-1-5386-5377-7},
	url = {https://ieeexplore.ieee.org/document/8419358/},
	doi = {10.1109/ICHI.2018.00025},
	abstract = {In typical data analysis projects in biology and healthcare, simpler predictive models, such as regressions and decision trees, enjoy more popularity than more complex and expressive ones, such as neural networks. One reason for this is that the functioning of simpler models is easier to explain, which greatly increases user acceptance. A neural network, on the contrary, is often regarded as a black box model, because its very strength in modeling complex interactions also makes its operation almost impossible to explain. Still, neural networks remain very interesting tools, since they have demonstrated promising performance in a variety of predictive tasks, such as medical image classiﬁcation and segmentation, as well as clinical event prediction, i.e., in the modeling of therapy decisions and survival time. In this work, we attempt to improve the explainability of neural networks applied in healthcare. We propose to apply the Layer-wise Relevance Propagation algorithm to explain clinical decisions proposed by deep modern neural networks. This algorithm is able to highlight the features that lead to the probabilistic prediction of therapy decisions for each individual patient. We evaluate the feature-oriented explanations generated by the algorithm with clinical experts. We show that the features, which are identiﬁed by the algorithm to be relevant, largely agree with clinical knowledge and guidelines. We believe that being able to explain machine learning based decisions greatly improves transparency and acceptance of neural network models applied in the clinical domain.},
	language = {en},
	urldate = {2023-11-27},
	booktitle = {2018 {IEEE} {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	publisher = {IEEE},
	author = {Yang, Yinchong and Tresp, Volker and Wunderle, Marius and Fasching, Peter A.},
	month = jun,
	year = {2018},
	pages = {152--162},
	file = {Yang et al. - 2018 - Explaining Therapy Predictions with Layer-Wise Rel.pdf:files/70/Yang et al. - 2018 - Explaining Therapy Predictions with Layer-Wise Rel.pdf:application/pdf},
}

@article{rieger_explainability_nodate,
	title = {Explainability for neural networks},
	language = {en},
	author = {Rieger, Laura},
	file = {Rieger - Explainability for neural networks.pdf:files/72/Rieger - Explainability for neural networks.pdf:application/pdf},
}

@article{samek_concept-level_nodate,
	title = {Concept-{Level} {Explainable} {AI}},
	language = {en},
	author = {Samek, Wojciech and Berlin, TU},
	file = {Samek and Berlin - Concept-Level Explainable AI.pdf:files/74/Samek and Berlin - Concept-Level Explainable AI.pdf:application/pdf},
}

@article{achtibat_attribution_2023-1,
	title = {From attribution maps to human-understandable explanations through {Concept} {Relevance} {Propagation}},
	volume = {5},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00711-8},
	doi = {10.1038/s42256-023-00711-8},
	abstract = {Abstract
            The field of explainable artificial intelligence (XAI) aims to bring transparency to today’s powerful but opaque deep learning models. While local XAI methods explain individual predictions in the form of attribution maps, thereby identifying ‘where’ important features occur (but not providing information about ‘what’ they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of method thus provide only partial insights and leave the burden of interpreting the model’s reasoning to the user. Here we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the ‘where’ and ‘what’ questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model’s representation and reasoning through concept atlases, concept-composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision-making.},
	language = {en},
	number = {9},
	urldate = {2023-12-07},
	journal = {Nat Mach Intell},
	author = {Achtibat, Reduan and Dreyer, Maximilian and Eisenbraun, Ilona and Bosse, Sebastian and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
	month = sep,
	year = {2023},
	pages = {1006--1019},
	file = {Achtibat et al. - 2023 - From attribution maps to human-understandable expl.pdf:files/76/Achtibat et al. - 2023 - From attribution maps to human-understandable expl.pdf:application/pdf},
}

@misc{bommer_finding_2023,
	title = {Finding the right {XAI} method -- {A} {Guide} for the {Evaluation} and {Ranking} of {Explainable} {AI} {Methods} in {Climate} {Science}},
	url = {http://arxiv.org/abs/2303.00652},
	abstract = {Explainable artiﬁcial intelligence (XAI) methods shed light on the predictions of deep neural networks (DNNs). Several diﬀerent approaches exist and have partly already been successfully applied in climate science. However, the often missing ground truth explanations complicate their evaluation and validation, subsequently compounding the choice of the XAI method. Therefore, in this work, we introduce XAI evaluation in the context of climate research and assess diﬀerent desired explanation properties, namely, robustness, faithfulness, randomization, complexity, and localization. To this end we build upon previous work and train a multi-layer perceptron (MLP) and a convolutional neural network (CNN) to predict the decade based on annual-mean temperature maps. Next, multiple local XAI methods are applied and their performance is quantiﬁed for each evaluation property and compared against a baseline test. Independent of the network type, we ﬁnd that the XAI methods Integrated Gradients, Layer-wise relevance propagation, and InputGradients exhibit considerable robustness, faithfulness, and complexity while sacriﬁcing randomization. The opposite is true for Gradient, SmoothGrad, NoiseGrad, and FusionGrad. Notably, explanations using input perturbations, such as SmoothGrad and Integrated Gradients, do not improve robustness and faithfulness, contrary to previous claims. Overall, our experiments oﬀer a comprehensive overview of diﬀerent properties of explanation methods in the climate science context and supports users in the selection of a suitable XAI method.},
	language = {en},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Bommer, Philine and Kretschmer, Marlene and Hedström, Anna and Bareeva, Dilyara and Höhne, Marina M.-C.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.00652 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 17 pages, 8 figure, under review},
	file = {Bommer et al. - 2023 - Finding the right XAI method -- A Guide for the Ev.pdf:files/78/Bommer et al. - 2023 - Finding the right XAI method -- A Guide for the Ev.pdf:application/pdf},
}

@article{yeh_enhancing_2019,
	title = {Enhancing achievement and interest in mathematics learning through {Math}-{Island}},
	volume = {14},
	issn = {1793-7078},
	url = {https://doi.org/10.1186/s41039-019-0100-9},
	doi = {10.1186/s41039-019-0100-9},
	abstract = {Conventional teacher-led instruction remains dominant in most elementary mathematics classrooms in Taiwan. Under such instruction, the teacher can rarely take care of all students. Many students may then continue to fall behind the standard of mathematics achievement and lose their interest in mathematics; they eventually give up on learning mathematics. In fact, students in Taiwan generally have lower interest in learning mathematics compared to many other regions/countries. Thus, how to enhance students’ mathematics achievement and interest are two major problems, especially for those low-achieving students. This paper describes how we designed a game-based learning environment, called Math-Island, by incorporating the mechanisms of a construction management game into the knowledge map of the elementary mathematics curriculum. We also report an experiment conducted with 215 elementary students for 2 years, from grade 2 to grade 3. In this experiment, in addition to teacher-led instruction in the classroom, students were directed to learn with Math-Island by using their own tablets at school and at home. As a result of this experiment, we found that there is an increase in students’ mathematics achievement, especially in the calculation and word problems. Moreover, the achievements of low-achieving students in the experimental school outperformed the low-achieving students in the control school (a control group in another school) in word problems. Moreover, both the low-achieving students and the high-achieving students in the experimental school maintained a rather high level of interest in mathematics and in the system.},
	number = {1},
	journal = {Research and Practice in Technology Enhanced Learning},
	author = {Yeh, Charles Y. C. and Cheng, Hercy N. H. and Chen, Zhi-Hong and Liao, Calvin C. Y. and Chan, Tak-Wai},
	month = mar,
	year = {2019},
	pages = {5},
}

@article{yeh_enhancing_2019-1,
	title = {Enhancing achievement and interest in mathematics learning through {Math}-{Island}},
	volume = {14},
	issn = {1793-7078},
	url = {https://doi.org/10.1186/s41039-019-0100-9},
	doi = {10.1186/s41039-019-0100-9},
	abstract = {Conventional teacher-led instruction remains dominant in most elementary mathematics classrooms in Taiwan. Under such instruction, the teacher can rarely take care of all students. Many students may then continue to fall behind the standard of mathematics achievement and lose their interest in mathematics; they eventually give up on learning mathematics. In fact, students in Taiwan generally have lower interest in learning mathematics compared to many other regions/countries. Thus, how to enhance students’ mathematics achievement and interest are two major problems, especially for those low-achieving students. This paper describes how we designed a game-based learning environment, called Math-Island, by incorporating the mechanisms of a construction management game into the knowledge map of the elementary mathematics curriculum. We also report an experiment conducted with 215 elementary students for 2 years, from grade 2 to grade 3. In this experiment, in addition to teacher-led instruction in the classroom, students were directed to learn with Math-Island by using their own tablets at school and at home. As a result of this experiment, we found that there is an increase in students’ mathematics achievement, especially in the calculation and word problems. Moreover, the achievements of low-achieving students in the experimental school outperformed the low-achieving students in the control school (a control group in another school) in word problems. Moreover, both the low-achieving students and the high-achieving students in the experimental school maintained a rather high level of interest in mathematics and in the system.},
	number = {1},
	journal = {Research and Practice in Technology Enhanced Learning},
	author = {Yeh, Charles Y. C. and Cheng, Hercy N. H. and Chen, Zhi-Hong and Liao, Calvin C. Y. and Chan, Tak-Wai},
	month = mar,
	year = {2019},
	pages = {5},
}

@misc{bommer_finding_2023-1,
	title = {Finding the right {XAI} method -- {A} {Guide} for the {Evaluation} and {Ranking} of {Explainable} {AI} {Methods} in {Climate} {Science}},
	url = {http://arxiv.org/abs/2303.00652},
	abstract = {Explainable artiﬁcial intelligence (XAI) methods shed light on the predictions of deep neural networks (DNNs). Several diﬀerent approaches exist and have partly already been successfully applied in climate science. However, the often missing ground truth explanations complicate their evaluation and validation, subsequently compounding the choice of the XAI method. Therefore, in this work, we introduce XAI evaluation in the context of climate research and assess diﬀerent desired explanation properties, namely, robustness, faithfulness, randomization, complexity, and localization. To this end we build upon previous work and train a multi-layer perceptron (MLP) and a convolutional neural network (CNN) to predict the decade based on annual-mean temperature maps. Next, multiple local XAI methods are applied and their performance is quantiﬁed for each evaluation property and compared against a baseline test. Independent of the network type, we ﬁnd that the XAI methods Integrated Gradients, Layer-wise relevance propagation, and InputGradients exhibit considerable robustness, faithfulness, and complexity while sacriﬁcing randomization. The opposite is true for Gradient, SmoothGrad, NoiseGrad, and FusionGrad. Notably, explanations using input perturbations, such as SmoothGrad and Integrated Gradients, do not improve robustness and faithfulness, contrary to previous claims. Overall, our experiments oﬀer a comprehensive overview of diﬀerent properties of explanation methods in the climate science context and supports users in the selection of a suitable XAI method.},
	language = {en},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Bommer, Philine and Kretschmer, Marlene and Hedström, Anna and Bareeva, Dilyara and Höhne, Marina M.-C.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.00652 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 17 pages, 8 figure, under review},
	file = {Bommer et al. - 2023 - Finding the right XAI method -- A Guide for the Ev.pdf:files/83/Bommer et al. - 2023 - Finding the right XAI method -- A Guide for the Ev.pdf:application/pdf},
}

@article{irion_ai_2021,
	title = {{AI} {Regulation} in the {European} {Union} and {Trade} {Law}: {How} {Can} {Accountability} of {AI} and a {High} {Level} of {Consumer} {Protection} {Prevail} over a {Trade} {Discipline} on {Source} {Code}?},
	issn = {1556-5068},
	shorttitle = {{AI} {Regulation} in the {European} {Union} and {Trade} {Law}},
	url = {https://www.ssrn.com/abstract=3786567},
	doi = {10.2139/ssrn.3786567},
	abstract = {Artificial Intelligence (AI) applications can bring many benefits for consumers, as well as influence consumer behaviour and the choices they make. On a large scale AI can profoundly transform consumer markets by, for example, enabling fully personalised consumer transactions on a population-wide scale. AI-powered consumer services rapidly diffuse across the global digital ecosystem thereby connecting consumers in the European Union (EU) to business operating from outside the EU. Individuals who are at the receiving end of AI systems must be reassured that these technologies operate in a way that respects fundamental and consumer rights.},
	language = {en},
	urldate = {2023-12-08},
	journal = {SSRN Journal},
	author = {Irion, Kristina},
	year = {2021},
	file = {Irion - 2021 - AI Regulation in the European Union and Trade Law.pdf:files/86/Irion - 2021 - AI Regulation in the European Union and Trade Law.pdf:application/pdf},
}

@article{moradi_assessment_2023,
	title = {Assessment of forest cover changes using multi-temporal {Landsat} observation},
	volume = {25},
	issn = {1573-2975},
	url = {https://doi.org/10.1007/s10668-021-02097-2},
	doi = {10.1007/s10668-021-02097-2},
	abstract = {Monitoring the changes in forest cover has become an important tool for forest management due to its impact on climate change, desertification, soil erosion, and flooding. The Zagros Mountains forest-steppe is a temperate broadleaf and mixed forest ecoregion in Western Asia, and the present study analyzes the changes in Zagros forests from 1986 to 2019 using remote sensing techniques. Three Landsat images, acquired in 1986, 2002, and 2019, were used to map land cover classification in the study area. Classification kappa coefficients were 85.2, 94.1, and 94.7 for three images, respectively. Focusing on the class level, the results show that between 1986 and 2002, the primary cause of forest loss in the study area was the conversion of 7.8 thousand ha of oak forests to agricultural land. Since 2002, 0.8 thousand ha of oak forest cover have been changed to barrenlands. The decrease of Zagros forests is a main threat to the rich biodiversity found in the West of Iran. To avoid worsening soil erosion, restoration programs should prioritize planting patterns on rainfall and hill slopes, texturally dissimilar soils, and regeneration in forest gaps, particularly at high elevations and steep slopes. It is recommended that future research should use higher resolution datasets and encompass more expansive landscape units, such as the whole Zagros Mountain forests.},
	number = {2},
	journal = {Environment, Development and Sustainability},
	author = {Moradi, Elahe and Sharifi, Alireza},
	month = feb,
	year = {2023},
	pages = {1351--1360},
}

@article{crespo_cuaresma_economic_2017,
	title = {Economic {Development} and {Forest} {Cover}: {Evidence} from {Satellite} {Data}},
	volume = {7},
	issn = {2045-2322},
	url = {https://doi.org/10.1038/srep40678},
	doi = {10.1038/srep40678},
	abstract = {Ongoing deforestation is a pressing, global environmental issue with direct impacts on climate change, carbon emissions, and biodiversity. There is an intuitive link between economic development and overexploitation of natural resources including forests, but this relationship has proven difficult to establish empirically due to both inadequate data and convoluting geo-climactic factors. In this analysis, we use satellite data on forest cover along national borders in order to study the determinants of deforestation differences across countries. Controlling for trans-border geo-climactic differences, we find that income per capita is the most robust determinant of differences in cross-border forest cover. We show that the marginal effect of per capita income growth on forest cover is strongest at the earliest stages of economic development, and weakens in more advanced economies, presenting some of the strongest evidence to date for the existence of at least half of an environmental Kuznets curve for deforestation.},
	number = {1},
	journal = {Scientific Reports},
	author = {Crespo Cuaresma, Jesús and Danylo, Olha and Fritz, Steffen and McCallum, Ian and Obersteiner, Michael and See, Linda and Walsh, Brian},
	month = jan,
	year = {2017},
	pages = {40678},
}

@article{saeed_explainable_2023,
	title = {Explainable {AI} ({XAI}): {A} systematic meta-survey of current challenges and future opportunities},
	volume = {263},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705123000230},
	doi = {https://doi.org/10.1016/j.knosys.2023.110273},
	abstract = {The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that have identified challenges and potential research directions of XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey of challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions of XAI and (2) challenges and research directions of XAI based on machine learning life cycle’s phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area.},
	journal = {Knowledge-Based Systems},
	author = {Saeed, Waddah and Omlin, Christian},
	year = {2023},
	keywords = {Black-box, Deep learning, Explainable AI (XAI), Interpretable AI, Machine learning, Meta-survey, Responsible AI},
	pages = {110273},
}

@article{sarker_machine_2021,
	title = {Machine {Learning}: {Algorithms}, {Real}-{World} {Applications} and {Research} {Directions}},
	volume = {2},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-021-00592-x},
	doi = {10.1007/s42979-021-00592-x},
	abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers in various real-world situations and application areas, particularly from the technical point of view.},
	number = {3},
	journal = {SN Computer Science},
	author = {Sarker, Iqbal H.},
	month = mar,
	year = {2021},
	pages = {160},
}

@article{alzubaidi_review_2021,
	title = {Review of deep learning: concepts, {CNN} architectures, challenges, applications, future directions},
	volume = {8},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-021-00444-8},
	doi = {10.1186/s40537-021-00444-8},
	abstract = {In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.},
	number = {1},
	journal = {Journal of Big Data},
	author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamaría, J. and Fadhel, Mohammed A. and Al-Amidie, Muthana and Farhan, Laith},
	month = mar,
	year = {2021},
	pages = {53},
}

@article{montavon_explaining_2017,
	title = {Explaining nonlinear classification decisions with deep {Taylor} decomposition},
	volume = {65},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320316303582},
	doi = {https://doi.org/10.1016/j.patcog.2016.11.008},
	abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
	journal = {Pattern Recognition},
	author = {Montavon, Grégoire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and Müller, Klaus-Robert},
	year = {2017},
	keywords = {Deep neural networks, Taylor decomposition, Heatmapping, Image recognition, Relevance propagation},
	pages = {211--222},
}

@article{bach_pixel-wise_2015-1,
	title = {On {Pixel}-{Wise} {Explanations} for {Non}-{Linear} {Classifier} {Decisions} by {Layer}-{Wise} {Relevance} {Propagation}},
	volume = {10},
	url = {https://doi.org/10.1371/journal.pone.0130140},
	doi = {10.1371/journal.pone.0130140},
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	number = {7},
	journal = {PLOS ONE},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
	month = jul,
	year = {2015},
	note = {Publisher: Public Library of Science},
	pages = {1--46},
}

@article{kakogeorgiou_evaluating_2021,
	title = {Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing},
	volume = {103},
	issn = {15698432},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0303243421002270},
	doi = {10.1016/j.jag.2021.102520},
	abstract = {Although deep neural networks hold the state-of-the-art in several remote sensing tasks, their black-box operation hinders the understanding of their decisions, concealing any bias and other shortcomings in datasets and model performance. To this end, we have applied explainable artificial intelligence (XAI) methods in remote sensing multi-label classification tasks towards producing human-interpretable explanations and improve transparency. In particular, we utilized and trained deep learning models with state-of-the-art performance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods were employed towards understanding and interpreting models' predictions, along with quantitative metrics to assess and compare their performance. Numerous experiments were performed to assess the overall performance of XAI methods for straightforward prediction cases, competing multiple labels, as well as misclassification cases. According to our findings, Occlusion, Grad-CAM and Lime were the most interpretable and reliable XAI methods. However, none delivers high-resolution outputs, while apart from Grad-CAM, both Lime and Occlusion are computationally expensive. We also highlight different aspects of XAI performance and elaborate with insights on black-box decisions in order to improve transparency, understand their behavior and reveal, as well, datasets’ particularities.},
	language = {en},
	urldate = {2023-12-28},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Kakogeorgiou, Ioannis and Karantzalos, Konstantinos},
	month = dec,
	year = {2021},
	pages = {102520},
	file = {Kakogeorgiou and Karantzalos - 2021 - Evaluating explainable artificial intelligence met.pdf:files/97/Kakogeorgiou and Karantzalos - 2021 - Evaluating explainable artificial intelligence met.pdf:application/pdf},
}

@misc{adebayo_sanity_2020,
	title = {Sanity {Checks} for {Saliency} {Maps}},
	url = {http://arxiv.org/abs/1810.03292},
	abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We ﬁnd that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, ﬁnding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our ﬁndings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental ﬁndings2.},
	language = {en},
	urldate = {2023-12-29},
	publisher = {arXiv},
	author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	month = nov,
	year = {2020},
	note = {arXiv:1810.03292 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Updating Guided Backprop experiments due to bug. The results and conclusions remain the same},
	file = {Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf:files/99/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf:application/pdf},
}
